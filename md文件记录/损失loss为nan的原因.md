# loss损失出现nan或者超级大的原因

## 现象

---

在模型训练或者预测过程中，经常会遇到训练损失值loss或者验证损失值loss不正常、无穷大或者为nan的情况：

Epoch 2/2 974/974 [==============================] - 1695s 2s/step - loss: nan - val_loss: nan

---

遇到这样的现象，主要有以下几个原因导致：

## 梯度爆炸造成loss爆炸：学习率过大

学习率过大可能导致最后的结果不会收敛；学习率过小会降低网络优化的速度，增加训练时间。

主要是学习率过大，影响到每次更新值的程度比较大，震荡幅度就会很大。过大的学习率会导致无法顺利到达最低点，稍有不慎就会跳出可控制区域，这样损失就会成倍增加。

此外，网络层数比较深的时候更容易发生。借用gluon的一段话：

此话通俗理解，以感知机y = w * x + b为例，偏置b=0,则输出y = x * w；假设所有层权重参数和输入为标量，如权重参数为0.2和5，以下面为例。![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5vbGRwYW4ubWUvZ3JhZGllbnRfYmxvdy5qcGc)

典型的梯度爆炸，解决方法就是降低初始学习率，并设置学习率衰减。

## 检查输入数据和输出数据

通常输入数据不对的话，能够很快就看到。

以下两种情况可能并不是那么容易检测到：

- 数据比较多，99%的数据是对的，但有1%的数据不正常，或者损坏，在训练过程中这些数据往往会造成nan或inf.
- 训练时跳出了错误的数据。

## 损失函数可能不正常

损失函数也是有可能的，尤其是自己设计的函数，应注意考虑是否可以正常地backward。

其次是对输入的Tensor是否进行了类型转化，保证计算中保持同一类型。

最后考虑在除数中加入微小的常数保证计算稳定性。

## batchNorm可能捣鬼

若网络中BatchNorm层数较多，可以适当检查下Tensor在输入BatchNorm层后有没有可能变为nan,若恰好发生这种情况，BatchNorm层中的移动均值和移动方差也很有可能都是nan，且这种情况可能发生在预测阶段。

## 采用stride大于kernel size的池化层

在卷积层的卷积步伐大于卷积核大小的时候，可能产生nan:

## shuffle设置问题

若无BatchNorm层，则训练阶段开启shuffle而在预测阶段关闭shuffle；若有BatchNorm层，且数据的分布极不规律，那么在训练阶段训练好的模型（使用shuffle），预测阶段时（不用shuffle），由于数据分布的不同，有可能导致batchnorm层出现nan.